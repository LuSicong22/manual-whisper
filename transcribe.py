#!/usr/bin/env python3
"""
ä¸­æ–‡ä¼šè®®å½•éŸ³è½¬å†™å·¥å…· (æœ¬åœ°ç‰ˆ)
ä½¿ç”¨ WhisperX æ¨¡å‹ï¼Œæ”¯æŒè¯´è¯äººåˆ†ç¦»å’Œæ—¶é—´æˆ³
"""

import whisperx
import gc
import torch
import json
import sys
import os
import re
import time
from pathlib import Path
from datetime import timedelta
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# Fix for PyTorch 2.6+ compatibility
_original_load = torch.load
def _safe_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return _original_load(*args, **kwargs)
torch.load = _safe_load

# ==================== é…ç½® ====================
DEVICE = "cpu"
BATCH_SIZE = int(os.environ.get("BATCH_SIZE", "4"))  # CPU æ¨è 4-8
COMPUTE_TYPE = "int8"
MODEL_SIZE = os.environ.get("WHISPER_MODEL", "large-v3")

# Hugging Face Token (ç”¨äºè¯´è¯äººåˆ†ç¦»)
HF_TOKEN = os.environ.get("HF_TOKEN")

# å¼•å¯¼ç®€ä½“ä¸­æ–‡è¾“å‡ºå’Œæ ‡ç‚¹
INITIAL_PROMPT = "ä»¥ä¸‹æ˜¯ä¸€æ®µä¸­æ–‡ä¼šè®®å½•éŸ³çš„è½¬å†™ã€‚è¯·ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚"

# VAD å‚æ•°ï¼ˆå‡å°‘å¹»è§‰ + åŠ é€Ÿï¼‰
VAD_OPTIONS = {
    "vad_onset": 0.5,
    "vad_offset": 0.363,
}


def format_timestamp(seconds):
    """å°†ç§’æ•°è½¬æ¢ä¸º HH:MM:SS æ ¼å¼"""
    td = timedelta(seconds=seconds)
    total_seconds = int(td.total_seconds())
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    secs = total_seconds % 60
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"


def remove_hallucination_loops(text, max_repeat=3):
    """ç§»é™¤é‡å¤çŸ­è¯­å¹»è§‰ï¼ˆå¦‚ 'é‚£ä¸ªå§ é‚£ä¸ªå§ é‚£ä¸ªå§ ...'ï¼‰"""
    # åŒ¹é…è¿ç»­é‡å¤ max_repeat æ¬¡ä»¥ä¸Šçš„çŸ­è¯­ï¼ˆ2-20 å­—ç¬¦ï¼‰
    pattern = r'(.{2,20}?)\1{' + str(max_repeat) + r',}'
    cleaned = re.sub(pattern, r'\1', text)
    return cleaned


def transcribe_audio(audio_file, hf_token=None):
    """è½¬å†™éŸ³é¢‘æ–‡ä»¶"""
    print(f"ğŸ™ï¸ æ­£åœ¨è½¬å†™: {audio_file}")
    print(f"ğŸ“Š é…ç½®: æ¨¡å‹={MODEL_SIZE} | è®¾å¤‡={DEVICE} | ç²¾åº¦={COMPUTE_TYPE} | æ‰¹å¤§å°={BATCH_SIZE}")

    step_start = time.time()

    # 1. åŠ è½½ Whisper æ¨¡å‹
    print(f"ğŸ“ åŠ è½½ Whisper {MODEL_SIZE} æ¨¡å‹...")
    model = whisperx.load_model(
        MODEL_SIZE, DEVICE,
        compute_type=COMPUTE_TYPE,
        language="zh",
        asr_options={"initial_prompt": INITIAL_PROMPT},
        vad_options=VAD_OPTIONS,
    )
    print(f"   æ¨¡å‹åŠ è½½è€—æ—¶: {time.time() - step_start:.1f}s")

    print("ğŸ”Š åŠ è½½éŸ³é¢‘...")
    audio = whisperx.load_audio(audio_file)
    audio_duration = len(audio) / 16000  # WhisperX é‡‡æ ·ç‡ 16kHz
    print(f"   éŸ³é¢‘æ—¶é•¿: {audio_duration:.0f}s ({audio_duration/60:.1f}min)")

    step_start = time.time()
    print("âœï¸ è½¬å†™ä¸­ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    result = model.transcribe(audio, batch_size=BATCH_SIZE, language="zh")
    transcribe_time = time.time() - step_start
    print(f"   è½¬å†™è€—æ—¶: {transcribe_time:.1f}s (å®æ—¶æ¯”: {transcribe_time/audio_duration:.1f}x)")

    # 2. å¯¹é½æ—¶é—´æˆ³
    step_start = time.time()
    print("ğŸ¯ å¯¹é½æ—¶é—´æˆ³...")
    model_a, metadata = whisperx.load_align_model(language_code="zh", device=DEVICE)
    result = whisperx.align(result["segments"], model_a, metadata, audio, DEVICE, return_char_alignments=False)
    print(f"   å¯¹é½è€—æ—¶: {time.time() - step_start:.1f}s")

    del model_a
    gc.collect()

    # 3. è¯´è¯äººåˆ†ç¦»
    if hf_token:
        step_start = time.time()
        print("ğŸ‘¥ è¯†åˆ«è¯´è¯äºº...")
        try:
            from whisperx.diarize import DiarizationPipeline
            diarize_model = DiarizationPipeline(use_auth_token=hf_token, device=DEVICE)
            diarize_segments = diarize_model(audio)
            result = whisperx.assign_word_speakers(diarize_segments, result)
            print(f"   è¯´è¯äººåˆ†ç¦»è€—æ—¶: {time.time() - step_start:.1f}s")
        except Exception as e:
            print(f"âš ï¸ è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
            print("   ç»§ç»­ç”Ÿæˆä¸å¸¦è¯´è¯äººæ ‡ç­¾çš„è½¬å†™ç¨¿...")
    else:
        print("âš ï¸ æœªè®¾ç½® HF_TOKENï¼Œè·³è¿‡è¯´è¯äººåˆ†ç¦»ã€‚è¯·åœ¨ .env ä¸­è®¾ç½® HF_TOKENã€‚")

    del model
    gc.collect()

    # 4. åå¤„ç†ï¼šç§»é™¤å¹»è§‰é‡å¤
    print("ğŸ§¹ æ¸…ç†å¹»è§‰é‡å¤...")
    hallucination_count = 0
    for segment in result.get("segments", []):
        original = segment.get("text", "")
        cleaned = remove_hallucination_loops(original)
        if cleaned != original:
            segment["text"] = cleaned
            hallucination_count += 1
    if hallucination_count > 0:
        print(f"   ä¿®å¤äº† {hallucination_count} å¤„å¹»è§‰é‡å¤")

    return result, audio_duration


def format_transcript(result, audio_file, output_file, audio_duration, total_time):
    """æ ¼å¼åŒ–è¾“å‡ºä¸º Markdown"""
    print(f"ğŸ“„ ç”Ÿæˆæ–‡æ¡£: {output_file}")

    lines = []
    lines.append("# ä¼šè®®å½•éŸ³è½¬å†™\n\n")
    lines.append(f"**æºæ–‡ä»¶**: {audio_file}  \n")
    lines.append(f"**éŸ³é¢‘æ—¶é•¿**: {audio_duration/60:.1f} åˆ†é’Ÿ  \n")
    lines.append(f"**æ¨¡å‹**: {MODEL_SIZE} | **ç²¾åº¦**: {COMPUTE_TYPE} | **è®¾å¤‡**: {DEVICE}  \n")
    lines.append(f"**è½¬å†™æ€»è€—æ—¶**: {total_time:.0f}s\n\n")
    lines.append("---\n\n")

    current_speaker = None

    for segment in result.get("segments", []):
        start = segment.get("start", 0)
        end = segment.get("end", 0)
        text = segment.get("text", "").strip()
        speaker = segment.get("speaker", "")

        if not text:
            continue

        timestamp = f"[{format_timestamp(start)} - {format_timestamp(end)}]"

        if speaker and speaker != current_speaker:
            lines.append(f"\n### {speaker}\n\n")
            current_speaker = speaker

        lines.append(f"{timestamp} {text}\n\n")

    with open(output_file, "w", encoding="utf-8") as f:
        f.writelines(lines)

    # ä¿å­˜ JSON
    json_file = output_file.replace(".md", ".json")
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"âœ… è½¬å†™ç¨¿: {output_file}")
    print(f"âœ… JSON: {json_file}")


def main():
    total_start = time.time()

    # é»˜è®¤æˆ–å‘½ä»¤è¡Œå‚æ•°
    audio_file = sys.argv[1] if len(sys.argv) > 1 else "New Recording 46.m4a"

    if not os.path.exists(audio_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        sys.exit(1)

    output_file = Path(audio_file).stem + "_transcript.md"

    result, audio_duration = transcribe_audio(audio_file, HF_TOKEN)

    total_time = time.time() - total_start
    format_transcript(result, audio_file, output_file, audio_duration, total_time)

    print(f"\nğŸ‰ è½¬å†™å®Œæˆ!")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}s | éŸ³é¢‘æ—¶é•¿: {audio_duration:.0f}s | å®æ—¶æ¯”: {total_time/audio_duration:.1f}x")


if __name__ == "__main__":
    main()
